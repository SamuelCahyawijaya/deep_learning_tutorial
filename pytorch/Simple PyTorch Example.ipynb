{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple pyTorch Example\n",
    "Please go to http://pytorch.org/docs/ for the package documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Common Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pyTorch Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Fully Connected Layer With Single Output\n",
    "<img src=\"http://cs231n.github.io/assets/nn1/neural_net2.jpeg\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "x_train = np.random.random((500, 3))\n",
    "y_train = np.mean(x_train, axis=-1)\n",
    "x_test = np.random.random((50, 3))\n",
    "y_test = np.mean(x_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape (500, 3)\n",
      "y_train.shape (500,)\n",
      "x_test.shape (50, 3)\n",
      "y_test.shape (50,)\n",
      "x_train[0] [ 0.5488135   0.71518937  0.60276338]\n",
      "y_train[0] 0.622255415457\n",
      "x_test[0] [ 0.44679332  0.83699037  0.22182403]\n",
      "y_test[0] 0.501869238723\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train.shape\",x_train.shape)\n",
    "print(\"y_train.shape\",y_train.shape)\n",
    "print(\"x_test.shape\",x_test.shape)\n",
    "print(\"y_test.shape\",y_test.shape)\n",
    "print(\"x_train[0]\",x_train[0])\n",
    "print(\"y_train[0]\",y_train[0])\n",
    "print(\"x_test[0]\",x_test[0])\n",
    "print(\"y_test[0]\",y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = Variable(torch.from_numpy(x_train).float())\n",
    "y_train = Variable(torch.from_numpy(y_train).float())\n",
    "x_test = Variable(torch.from_numpy(x_test).float())\n",
    "y_test = Variable(torch.from_numpy(y_test).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=3, out_features=4, bias=True)\n",
      "  (fc2): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (fc3): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (fc4): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (fc5): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (fc6): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (fc7): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(3, 4)\n",
    "        self.fc2 = nn.Linear(4, 4)\n",
    "        self.fc3 = nn.Linear(4, 4)\n",
    "        self.fc4 = nn.Linear(4, 4)\n",
    "        self.fc5 = nn.Linear(4, 4)\n",
    "        self.fc6 = nn.Linear(4, 4)\n",
    "        self.fc7 = nn.Linear(4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.tanh(self.fc7(x))\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train & Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, train loss = 0.60187 test loss = 0.68273\n",
      "Epoch = 2, train loss = 0.56501 test loss = 0.64397\n",
      "Epoch = 3, train loss = 0.52864 test loss = 0.60502\n",
      "Epoch = 4, train loss = 0.49635 test loss = 0.57043\n",
      "Epoch = 5, train loss = 0.46684 test loss = 0.53883\n",
      "Epoch = 6, train loss = 0.43908 test loss = 0.50902\n",
      "Epoch = 7, train loss = 0.41562 test loss = 0.48375\n",
      "Epoch = 8, train loss = 0.39454 test loss = 0.46109\n",
      "Epoch = 9, train loss = 0.37405 test loss = 0.43901\n",
      "Epoch = 10, train loss = 0.35404 test loss = 0.41738\n",
      "Epoch = 11, train loss = 0.33446 test loss = 0.39618\n",
      "Epoch = 12, train loss = 0.31531 test loss = 0.37540\n",
      "Epoch = 13, train loss = 0.29663 test loss = 0.35506\n",
      "Epoch = 14, train loss = 0.27844 test loss = 0.33521\n",
      "Epoch = 15, train loss = 0.26080 test loss = 0.31589\n",
      "Epoch = 16, train loss = 0.24375 test loss = 0.29717\n",
      "Epoch = 17, train loss = 0.22735 test loss = 0.27908\n",
      "Epoch = 18, train loss = 0.21163 test loss = 0.26169\n",
      "Epoch = 19, train loss = 0.19665 test loss = 0.24503\n",
      "Epoch = 20, train loss = 0.18242 test loss = 0.22915\n",
      "Epoch = 21, train loss = 0.16897 test loss = 0.21406\n",
      "Epoch = 22, train loss = 0.15632 test loss = 0.19979\n",
      "Epoch = 23, train loss = 0.14447 test loss = 0.18635\n",
      "Epoch = 24, train loss = 0.13343 test loss = 0.17375\n",
      "Epoch = 25, train loss = 0.12317 test loss = 0.16196\n",
      "Epoch = 26, train loss = 0.11369 test loss = 0.15100\n",
      "Epoch = 27, train loss = 0.10496 test loss = 0.14082\n",
      "Epoch = 28, train loss = 0.09696 test loss = 0.13142\n",
      "Epoch = 29, train loss = 0.08966 test loss = 0.12276\n",
      "Epoch = 30, train loss = 0.08302 test loss = 0.11481\n",
      "Epoch = 31, train loss = 0.07700 test loss = 0.10753\n",
      "Epoch = 32, train loss = 0.07157 test loss = 0.10089\n",
      "Epoch = 33, train loss = 0.06668 test loss = 0.09484\n",
      "Epoch = 34, train loss = 0.06231 test loss = 0.08935\n",
      "Epoch = 35, train loss = 0.05840 test loss = 0.08439\n",
      "Epoch = 36, train loss = 0.05492 test loss = 0.07990\n",
      "Epoch = 37, train loss = 0.05183 test loss = 0.07585\n",
      "Epoch = 38, train loss = 0.04910 test loss = 0.07221\n",
      "Epoch = 39, train loss = 0.04669 test loss = 0.06894\n",
      "Epoch = 40, train loss = 0.04457 test loss = 0.06600\n",
      "Epoch = 41, train loss = 0.04271 test loss = 0.06337\n",
      "Epoch = 42, train loss = 0.04108 test loss = 0.06102\n",
      "Epoch = 43, train loss = 0.03967 test loss = 0.05892\n",
      "Epoch = 44, train loss = 0.03843 test loss = 0.05705\n",
      "Epoch = 45, train loss = 0.03736 test loss = 0.05538\n",
      "Epoch = 46, train loss = 0.03644 test loss = 0.05389\n",
      "Epoch = 47, train loss = 0.03564 test loss = 0.05256\n",
      "Epoch = 48, train loss = 0.03495 test loss = 0.05138\n",
      "Epoch = 49, train loss = 0.03435 test loss = 0.05033\n",
      "Epoch = 50, train loss = 0.03385 test loss = 0.04940\n",
      "Epoch = 51, train loss = 0.03341 test loss = 0.04857\n",
      "Epoch = 52, train loss = 0.03304 test loss = 0.04783\n",
      "Epoch = 53, train loss = 0.03273 test loss = 0.04717\n",
      "Epoch = 54, train loss = 0.03246 test loss = 0.04658\n",
      "Epoch = 55, train loss = 0.03223 test loss = 0.04606\n",
      "Epoch = 56, train loss = 0.03204 test loss = 0.04559\n",
      "Epoch = 57, train loss = 0.03188 test loss = 0.04518\n",
      "Epoch = 58, train loss = 0.03174 test loss = 0.04481\n",
      "Epoch = 59, train loss = 0.03163 test loss = 0.04448\n",
      "Epoch = 60, train loss = 0.03153 test loss = 0.04419\n",
      "Epoch = 61, train loss = 0.03145 test loss = 0.04392\n",
      "Epoch = 62, train loss = 0.03138 test loss = 0.04369\n",
      "Epoch = 63, train loss = 0.03133 test loss = 0.04348\n",
      "Epoch = 64, train loss = 0.03128 test loss = 0.04330\n",
      "Epoch = 65, train loss = 0.03124 test loss = 0.04313\n",
      "Epoch = 66, train loss = 0.03121 test loss = 0.04298\n",
      "Epoch = 67, train loss = 0.03118 test loss = 0.04285\n",
      "Epoch = 68, train loss = 0.03116 test loss = 0.04273\n",
      "Epoch = 69, train loss = 0.03114 test loss = 0.04262\n",
      "Epoch = 70, train loss = 0.03113 test loss = 0.04252\n",
      "Epoch = 71, train loss = 0.03112 test loss = 0.04244\n",
      "Epoch = 72, train loss = 0.03111 test loss = 0.04236\n",
      "Epoch = 73, train loss = 0.03110 test loss = 0.04229\n",
      "Epoch = 74, train loss = 0.03109 test loss = 0.04223\n",
      "Epoch = 75, train loss = 0.03109 test loss = 0.04218\n",
      "Epoch = 76, train loss = 0.03108 test loss = 0.04213\n",
      "Epoch = 77, train loss = 0.03108 test loss = 0.04208\n",
      "Epoch = 78, train loss = 0.03108 test loss = 0.04204\n",
      "Epoch = 79, train loss = 0.03108 test loss = 0.04201\n",
      "Epoch = 80, train loss = 0.03107 test loss = 0.04198\n",
      "Epoch = 81, train loss = 0.03107 test loss = 0.04195\n",
      "Epoch = 82, train loss = 0.03107 test loss = 0.04192\n",
      "Epoch = 83, train loss = 0.03107 test loss = 0.04190\n",
      "Epoch = 84, train loss = 0.03107 test loss = 0.04188\n",
      "Epoch = 85, train loss = 0.03107 test loss = 0.04186\n",
      "Epoch = 86, train loss = 0.03107 test loss = 0.04185\n",
      "Epoch = 87, train loss = 0.03107 test loss = 0.04183\n",
      "Epoch = 88, train loss = 0.03107 test loss = 0.04182\n",
      "Epoch = 89, train loss = 0.03107 test loss = 0.04181\n",
      "Epoch = 90, train loss = 0.03107 test loss = 0.04180\n",
      "Epoch = 91, train loss = 0.03107 test loss = 0.04179\n",
      "Epoch = 92, train loss = 0.03107 test loss = 0.04179\n",
      "Epoch = 93, train loss = 0.03107 test loss = 0.04178\n",
      "Epoch = 94, train loss = 0.03107 test loss = 0.04177\n",
      "Epoch = 95, train loss = 0.03107 test loss = 0.04177\n",
      "Epoch = 96, train loss = 0.03107 test loss = 0.04176\n",
      "Epoch = 97, train loss = 0.03107 test loss = 0.04176\n",
      "Epoch = 98, train loss = 0.03107 test loss = 0.04176\n",
      "Epoch = 99, train loss = 0.03107 test loss = 0.04175\n",
      "Epoch = 100, train loss = 0.03107 test loss = 0.04175\n",
      "Epoch = 101, train loss = 0.03107 test loss = 0.04175\n",
      "Epoch = 102, train loss = 0.03107 test loss = 0.04175\n",
      "Epoch = 103, train loss = 0.03107 test loss = 0.04174\n",
      "Epoch = 104, train loss = 0.03107 test loss = 0.04174\n",
      "Epoch = 105, train loss = 0.03107 test loss = 0.04174\n",
      "Epoch = 106, train loss = 0.03107 test loss = 0.04174\n",
      "Epoch = 107, train loss = 0.03107 test loss = 0.04174\n",
      "Epoch = 108, train loss = 0.03107 test loss = 0.04174\n",
      "Epoch = 109, train loss = 0.03107 test loss = 0.04174\n",
      "Epoch = 110, train loss = 0.03107 test loss = 0.04174\n",
      "Epoch = 111, train loss = 0.03107 test loss = 0.04174\n",
      "Epoch = 112, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 113, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 114, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 115, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 116, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 117, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 118, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 119, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 120, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 121, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 122, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 123, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 124, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 125, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 126, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 127, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 128, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 129, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 130, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 131, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 132, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 133, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 134, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 135, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 136, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 137, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 138, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 139, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 140, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 141, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 142, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 143, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 144, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 145, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 146, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 147, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 148, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 149, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 150, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 151, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 152, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 153, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 154, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 155, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 156, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 157, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 158, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 159, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 160, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 161, train loss = 0.03107 test loss = 0.04173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 162, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 163, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 164, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 165, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 166, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 167, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 168, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 169, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 170, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 171, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 172, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 173, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 174, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 175, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 176, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 177, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 178, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 179, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 180, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 181, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 182, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 183, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 184, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 185, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 186, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 187, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 188, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 189, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 190, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 191, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 192, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 193, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 194, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 195, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 196, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 197, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 198, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 199, train loss = 0.03107 test loss = 0.04173\n",
      "Epoch = 200, train loss = 0.03107 test loss = 0.04173\n",
      "y_train[0:5], yp_train[0:5]\n",
      "Variable containing:\n",
      " 0.6223\n",
      "[torch.FloatTensor of size 1]\n",
      " Variable containing:\n",
      " 0.5010\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.5381\n",
      "[torch.FloatTensor of size 1]\n",
      " Variable containing:\n",
      " 0.5010\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.7643\n",
      "[torch.FloatTensor of size 1]\n",
      " Variable containing:\n",
      " 0.5010\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.5680\n",
      "[torch.FloatTensor of size 1]\n",
      " Variable containing:\n",
      " 0.5010\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.5216\n",
      "[torch.FloatTensor of size 1]\n",
      " Variable containing:\n",
      " 0.5010\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "y_test[0:5], yp_test[0:5]\n",
      "Variable containing:\n",
      " 0.5019\n",
      "[torch.FloatTensor of size 1]\n",
      " Variable containing:\n",
      " 0.5010\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.6969\n",
      "[torch.FloatTensor of size 1]\n",
      " Variable containing:\n",
      " 0.5010\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.7765\n",
      "[torch.FloatTensor of size 1]\n",
      " Variable containing:\n",
      " 0.5010\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.2774\n",
      "[torch.FloatTensor of size 1]\n",
      " Variable containing:\n",
      " 0.5010\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.5927\n",
      "[torch.FloatTensor of size 1]\n",
      " Variable containing:\n",
      " 0.5010\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "steps = int(np.ceil(x_train.shape[0] / batch_size))\n",
    "\n",
    "for epoch in range(200):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    for i in range(steps):\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        \n",
    "        output = net(x_train[i*batch_size:(i+1)*batch_size])\n",
    "        loss = criterion(output, y_train[i*batch_size:(i+1)*batch_size])\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()    # Does the update\n",
    "    \n",
    "    # Compute and print loss.\n",
    "    yp_train = net(x_train)\n",
    "    yp_test = net(x_test)\n",
    "    \n",
    "    loss_train = criterion(yp_train, y_train)\n",
    "    loss_test = criterion(yp_test, y_test)\n",
    "\n",
    "    print(\"Epoch = %d, train loss = %.5f test loss = %.5f\" % (epoch + 1, loss_train, loss_test))\n",
    "\n",
    "\n",
    "print(\"y_train[0:5], yp_train[0:5]\")\n",
    "for i in range(5):\n",
    "    print(y_train[i], yp_train[i])\n",
    "    \n",
    "print(\"y_test[0:5], yp_test[0:5]\")\n",
    "for i in range(5):\n",
    "    print(y_test[i], yp_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
